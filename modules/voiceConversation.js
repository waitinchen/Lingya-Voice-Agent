/**
 * å®Œæ•´èªéŸ³å°è©±æµç¨‹æ¨¡çµ„
 * Step â‘¢-Aï¼šæ•´åˆèªéŸ³è¼¸å…¥ â†’ æ–‡å­—ç†è§£ â†’ AIå›æ‡‰ â†’ èªéŸ³è¼¸å‡º
 * 
 * æµç¨‹ï¼šéŸ³é »è¼¸å…¥ â†’ STT â†’ LLM â†’ TTS â†’ éŸ³é »è¼¸å‡º
 */

import { transcribeAudio, transcribeFromBase64 } from "./stt.js";
import fs from "fs";
import path from "path";
import { chatWithLLM, analyzeEmotion } from "./llm.js";
import { synthesizeSpeechCartesiaToBuffer } from "./tts-cartesia.js";

/**
 * è™•ç†å®Œæ•´çš„èªéŸ³å°è©±æµç¨‹ï¼ˆæ”¯æŒæ­¸å±¬è¨˜æ†¶ï¼‰
 * @param {Buffer|Blob|string} audioInput - éŸ³é »è¼¸å…¥ï¼ˆBufferã€Blob æˆ– Base64 å­—ä¸²ï¼‰
 * @param {Object} options - é¸é …
 * @param {string} options.language - èªè¨€ï¼ˆ'zh', 'en' ç­‰ï¼‰
 * @param {Array} options.history - å°è©±æ­·å²
 * @param {boolean} options.returnAudio - æ˜¯å¦è¿”å›éŸ³é »ï¼ˆé è¨­ trueï¼‰
 * @param {string} options.userIdentity - ç”¨æˆ¶èº«ä»½ï¼ˆ'dad'/'è€çˆ¸' è¡¨ç¤ºé™³å¨å»·ï¼‰
 * @param {string} options.userName - ç”¨æˆ¶åç¨±
 * @returns {Promise<Object>} åŒ…å«æ–‡å­—å’ŒéŸ³é »çš„å›æ‡‰
 */
export async function processVoiceConversation(audioInput, options = {}) {
  try {
    const {
      language = "zh",
      history = [],
      returnAudio = true,
      userIdentity,
      userName,
    } = options;

    // Step 1: èªéŸ³è­˜åˆ¥ (STT)
    console.log("ğŸ¤ Step 1: èªéŸ³è­˜åˆ¥...");
    let transcribedText;
    
    if (typeof audioInput === "string") {
      // æª¢æŸ¥æ˜¯æ–‡ä»¶è·¯å¾‘é‚„æ˜¯ Base64
      if (audioInput.includes("/") || audioInput.includes("\\")) {
        // æ–‡ä»¶è·¯å¾‘
        const fileName = path.basename(audioInput);
        transcribedText = await transcribeAudio(audioInput, { 
          language,
          fileName: fileName, // ä¿ç•™åŸå§‹æ–‡ä»¶åå’Œæ“´å±•å
        });
      } else {
        // Base64 å­—ä¸²
        transcribedText = await transcribeFromBase64(audioInput, { language });
      }
    } else {
      // Buffer
      transcribedText = await transcribeAudio(audioInput, { language });
    }

    if (!transcribedText || transcribedText.trim().length === 0) {
      return {
        success: false,
        error: "æœªè­˜åˆ¥åˆ°èªéŸ³ã€‚è«‹ç¢ºä¿ï¼š1) éŒ„éŸ³æ™‚é•·è‡³å°‘ 0.5 ç§’ï¼›2) èªªè©±è²éŸ³æ¸…æ™°ï¼›3) éº¥å…‹é¢¨æ¬Šé™å·²å…è¨±ã€‚",
        text: "",
        audio: null,
      };
    }

    console.log(`ğŸ“ ç”¨æˆ¶èªª: "${transcribedText}"`);

    // Step 1.5: åˆ†ææƒ…ç·’ï¼ˆStep â‘¢-B æ–°å¢ï¼‰
    console.log("ğŸ˜Š Step 1.5: åˆ†ææƒ…ç·’...");
    const emotion = await analyzeEmotion(transcribedText);
    console.log(`   æª¢æ¸¬åˆ°æƒ…ç·’: ${emotion}`);

    // Step 2: LLM ç”Ÿæˆå›æ‡‰ï¼ˆä½¿ç”¨å°è©±æ­·å²å’Œæƒ…ç·’ï¼Œæ”¯æŒæ¨™ç±¤é¸æ“‡å’Œèº«ä»½è­˜åˆ¥ï¼‰
    console.log(`ğŸ¤– Step 2: AI ç”Ÿæˆå›æ‡‰ï¼ˆèº«ä»½: ${userIdentity || "æœªçŸ¥"}, æƒ…æ„Ÿ: ${emotion}ï¼‰...`);
    const llmResult = await chatWithLLM(transcribedText, history, {
      emotion,
      isVoice: true, // æ¨™è¨˜é€™æ˜¯èªéŸ³è¼¸å…¥
      enableTags: true, // å•Ÿç”¨æƒ…ç·’æ¨™ç±¤é¸æ“‡
      userIdentity, // å‚³éç”¨æˆ¶èº«ä»½ï¼ˆæ­¸å±¬è¨˜æ†¶æ ¸å¿ƒï¼‰
      userName, // å‚³éç”¨æˆ¶åç¨±
    });
    
    const replyText = llmResult.reply;
    const selectedTags = llmResult.tags || [];
    
    console.log(`ğŸ’¬ èŠ±å°è»Ÿå›æ‡‰: "${replyText}"`);
    if (selectedTags.length > 0) {
      console.log(`ğŸ·ï¸  é¸æ“‡çš„èªæ°£æ¨™ç±¤: [${selectedTags.join(", ")}]`);
    }

    // Step 3: èªéŸ³åˆæˆ (TTS) - ä½¿ç”¨é¸æ“‡çš„æ¨™ç±¤ï¼ˆStep â‘¢-B å¢å¼·ï¼‰
    let audioBuffer = null;
    if (returnAudio) {
      console.log(`ğŸ”Š Step 3: èªéŸ³åˆæˆï¼ˆæ¨™ç±¤: [${selectedTags.join(", ") || "ç„¡"}]ï¼‰...`);
      audioBuffer = await synthesizeSpeechCartesiaToBuffer(replyText, {
        tags: selectedTags, // ä½¿ç”¨ LLM é¸æ“‡çš„æ¨™ç±¤
        emotion, // ä¿ç•™èˆŠç‰ˆåƒæ•¸ä»¥å‚™ç”¨
      });
      
      if (audioBuffer) {
        console.log(`âœ… èªéŸ³ç”Ÿæˆå®Œæˆï¼ˆæ¨™ç±¤: [${selectedTags.join(", ") || "ç„¡"}]ï¼‰ï¼Œå¤§å°: ${(audioBuffer.length / 1024).toFixed(2)} KB`);
      }
    }

    // æ›´æ–°å°è©±æ­·å²
    const updatedHistory = [
      ...history,
      { role: "user", content: transcribedText },
      { role: "assistant", content: replyText },
    ];

    return {
      success: true,
      text: replyText,
      transcribedText,
      audio: audioBuffer ? audioBuffer.toString("base64") : null,
      history: updatedHistory,
      emotion, // è¿”å›æª¢æ¸¬åˆ°çš„æƒ…ç·’
      tags: selectedTags, // Step â‘¢-B: è¿”å›é¸æ“‡çš„æ¨™ç±¤
    };
  } catch (error) {
    console.error("âŒ èªéŸ³å°è©±è™•ç†å¤±æ•—:", error);
    return {
      success: false,
      error: error.message,
      text: "",
      audio: null,
    };
  }
}

